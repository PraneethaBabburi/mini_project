{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mdk-Fv7QET51"
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasRegressor, KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.constraints import maxnorm\n",
    "\n",
    "\n",
    "class Deep_Models:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.model = Sequential()\n",
    "        self.estimators = []\n",
    "        self.hidden_layers = [100, 100, 100]\n",
    "        self.input_dm = 1\n",
    "        self.activation_fn = ['relu']\n",
    "        self.k_fold = 10\n",
    "        self.loss = 'binary_crossentropy'\n",
    "        self.optimizer = 'adam'\n",
    "        self.model_type = 'classifier'\n",
    "        self.no_of_output = 1\n",
    "        self.metrics = ['accuracy']\n",
    "        self.sample_weight = None\n",
    "        self.dropout_spec = [0.2, 0.2, 0.2]\n",
    "\n",
    "    def get_name(self):\n",
    "        return self.name\n",
    "\n",
    "    def get_activation_fn(self, activation_fn, pos):\n",
    "        if len(activation_fn) - 2 < pos:\n",
    "            return activation_fn[-2]\n",
    "        else:\n",
    "            return activation_fn[pos]\n",
    "\n",
    "    def update_parameters(self, param):\n",
    "        self.hidden_layers = param['hidden_layers']\n",
    "        self.input_dm = param['input_dm']\n",
    "        self.activation_fn = param['activation_fn']\n",
    "        self.k_fold = param['k_fold']\n",
    "        self.loss = param['loss']\n",
    "        self.optimizer = param['optimizer']\n",
    "        self.model_type = param['model_type']\n",
    "        self.no_of_output = param['no_of_output']\n",
    "        self.metrics = param['metrics']\n",
    "        self.sample_weight = param['sample_weight']\n",
    "        self.dropout_spec = param['dropout_spec']\n",
    "\n",
    "    def update_parameters_class_Weight(self, param):\n",
    "        self.hidden_layers = param['hidden_layers']\n",
    "        self.input_dm = param['input_dm']\n",
    "        self.activation_fn = param['activation_fn']\n",
    "        self.k_fold = param['k_fold']\n",
    "        self.loss = param['loss']\n",
    "        self.optimizer = param['optimizer']\n",
    "        self.model_type = param['model_type']\n",
    "        self.no_of_output = param['no_of_output']\n",
    "        self.metrics = param['metrics']\n",
    "        self.sample_weight = param['sample_weight']\n",
    "        self.dropout_spec = param['dropout_spec']\n",
    "        self.class_weight = param['class_weight']\n",
    "\n",
    "    def build_model(self):\n",
    "\n",
    "        for hl in self.hidden_layers:\n",
    "            if self.hidden_layers.index(hl) == 0:  # adding the very first hidden layer\n",
    "                self.model.add(\n",
    "                    Dense(hl, input_dim=self.input_dm, kernel_initializer='normal', activation=self.activation_fn[0]))\n",
    "            else:\n",
    "                self.model.add(Dense(hl, kernel_initializer='normal',\n",
    "                                     activation=self.get_activation_fn(self.activation_fn,\n",
    "                                                                       self.hidden_layers.index(hl))))\n",
    "                self.model.add(Dropout(self.dropout_spec[self.hidden_layers.index(hl)]))\n",
    "\n",
    "        self.model.add(Dense(self.no_of_output, kernel_initializer='normal', activation=self.activation_fn[-1]))\n",
    "\n",
    "        self.model.compile(loss=self.loss, optimizer=self.optimizer, metrics=self.metrics)\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    def tune_model_Dropout(self, dropout_rate=0.0, weight_constraint=0):\n",
    "        for hl in self.hidden_layers:\n",
    "            if self.hidden_layers.index(hl) == 0:  # adding the very first hidden layer\n",
    "                self.model.add(\n",
    "                    Dense(hl, input_dim=self.input_dm, kernel_initializer='normal', activation=self.activation_fn[0],\n",
    "                          kernel_constraint=maxnorm(weight_constraint)))\n",
    "                self.model.add(Dropout(dropout_rate))\n",
    "            else:\n",
    "                self.model.add(Dense(hl, kernel_initializer='normal',\n",
    "                                     activation=self.get_activation_fn(self.activation_fn,\n",
    "                                                                       self.hidden_layers.index(hl))))\n",
    "                self.model.add(Dropout(dropout_rate))\n",
    "\n",
    "        self.model.add(Dense(self.no_of_output, kernel_initializer='normal', activation=self.activation_fn[-1]))\n",
    "\n",
    "        self.model.compile(loss=self.loss, optimizer=self.optimizer, metrics=self.metrics)\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    def build_estimator(self):\n",
    "        self.estimators.append(('standardize', StandardScaler()))\n",
    "\n",
    "        if self.model_type == 'regressor':\n",
    "            self.estimators.append(\n",
    "                ('mlp', KerasRegressor(build_fn=self.build_model, epochs=50, batch_size=5, verbose=0)))\n",
    "        elif self.model_type == 'classifier':\n",
    "            self.estimators.append(\n",
    "                ('mlp', KerasClassifier(build_fn=self.build_model, epochs=200, batch_size=5, verbose=0)))\n",
    "\n",
    "        self.pipeline = Pipeline(self.estimators)\n",
    "\n",
    "    def kfold_CVS(self, X, Y):\n",
    "        kfold = KFold(n_splits=10)\n",
    "        results = cross_val_score(self.pipeline, X, Y, cv=kfold)\n",
    "        print(\"Larger: %.2f (%.2f) MSE\" % (results.mean(), results.std()))\n",
    "\n",
    "    def DNN_Models(self, X, Y, **param):\n",
    "\n",
    "        self.update_parameters(param)\n",
    "        self.build_estimator()\n",
    "        if self.k_fold > 0:\n",
    "            self.kfold_CVS(X, Y)\n",
    "        print(self.sample_weight)\n",
    "        self.pipeline.fit(X, Y, **{'mlp__sample_weight': self.sample_weight})\n",
    "\n",
    "        return self.pipeline\n",
    "\n",
    "    def DNN_Models_Class_Weight(self, X, Y, **param):\n",
    "\n",
    "        self.update_parameters(param)\n",
    "        self.build_estimator()\n",
    "        if self.k_fold > 0:\n",
    "            self.kfold_CVS(X, Y)\n",
    "        print(self.sample_weight)\n",
    "        self.pipeline.fit(X, Y, **{'mlp__class_weight': self.sample_weight})\n",
    "\n",
    "        return self.pipeline\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
